---
- hosts:
    - all
    - localhost
  tags: [teardown]
  tasks:
    - name: verify the OS distribution
      debug:
        msg: We currently support hosts running with 'Debian' or 'Ubuntu' distribution.
      when: ansible_distribution not in ['Debian', 'Ubuntu']
      failed_when: ansible_distribution not in ['Debian', 'Ubuntu']

- hosts: all
  tags: [teardown]
  tasks:
    - name: verify inventory_hostname
      debug:
        msg: '{{ inventory_hostname }} is not a hostname'
      when: inventory_hostname | ipaddr
      failed_when: inventory_hostname | ipaddr

    - name: verify ansible_host
      debug:
        msg: '{{ ansible_host }} is not a valid IP address ({{ inventory_hostname }})'
      when: not ansible_host | ipaddr
      failed_when: not ansible_host | ipaddr

    # https://docs.ceph.com/docs/mimic/rados/operations/operating/#stopping-all-daemons
    - name: stop all ceph daemons # noqa 303
      become: yes
      shell: systemctl stop ceph\*.service ceph\*.target
      changed_when: True

    - name: remove Ceph systemd unit files # noqa 302
      become: yes
      shell: rm -rf /etc/systemd/system/ceph*
      args:
        warn: no
      changed_when: True

- hosts: osds
  tags: [teardown]
  tasks:
    - name: zap devices
      become: yes
      command: ceph-volume lvm zap --destroy {{ data_disks | join(' ') }}
      register: zap_dev_res
      failed_when: zap_dev_res.rc != 0 and zap_dev_res.rc != 2
      changed_when: zap_dev_res.rc == 0

    - import_role:
        name: rm-lvm-config
      when: zap_dev_res.rc != 0

- name: set up installation environment
  hosts: localhost
  tags: [teardown]
  tasks:
    - name: install Python virtualenv package
      become: yes
      apt:
        cache_valid_time: '{{ 60 * 60 * 24 }}' # cache is valid in a day
        install_recommends: no
        name: virtualenv
        state: present
        update_cache: yes

    - name: install ceph-deploy
      pip:
        name: ceph-deploy
        state: present
        version: 2.0.1
        virtualenv: '{{ workdir }}/ceph-deploy'
        virtualenv_python: python3.5

    - name: set ceph-deploy binary path
      set_fact:
        ceph_deploy: '{{ workdir }}/ceph-deploy/bin/ceph-deploy'

    - name: copy cephdeploy.conf to workdir
      copy:
        backup: no
        dest: '{{ workdir }}'
        force: yes
        src: cephdeploy.conf

    - name: add hostnames to the local /etc/hosts file
      become: yes
      lineinfile:
        backup: yes
        create: no
        line: "{{ hostvars[item]['ansible_host'] }} {{ item }}  # added by MBWU-Ceph"
        path: /etc/hosts
        regexp: "^{{ hostvars[item]['ansible_host'] }} "
        state: present
      loop: "{{ groups['all'] }}"

    # add this step to ensure that the next task "ceph-deploy purge" either finds
    # or does not find the packages to be removed.
    - name: update apt-get update
      apt:
        update_cache: yes
        cache_valid_time: '{{ 60 * 60 * 24 }}' # cache is valid in a day

    - name: remove Ceph packages from all hosts and purge all data
      command: >
        {{ ceph_deploy }} --username {{ hostvars[item]['ansible_user'] }}
        purge {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['all'] }}"
      changed_when: True

    - name: purge any Ceph data from /var/lib/ceph
      command: >
        {{ ceph_deploy }} --username {{ hostvars[item]['ansible_user'] }}
        purgedata {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['all'] }}"
      changed_when: True

    - name: remove authentication keys from the local directory
      command: '{{ ceph_deploy }} forgetkeys'
      args:
        chdir: '{{ workdir }}'
      changed_when: True

- hosts: all
  serial: 1
  tags: [never, cleanup]
  tasks:
    # this list is retrieved from file
    # https://chacra.ceph.com/r/ceph/nautilus/3d58626ebeec02d8385a4cefb92c6cbc3a45bfe8/ubuntu/xenial/flavors/default/dists/xenial/main/binary-amd64/Packages
    - name: remove dependent Ceph packages
      become: yes
      apt:
        autoremove: yes
        name:
          - ceph
          - ceph-base
          - ceph-base-dbg
          - ceph-common
          - ceph-common-dbg
          - ceph-deploy
          - ceph-fuse
          - ceph-fuse-dbg
          - ceph-mds
          - ceph-mds-dbg
          - ceph-mgr
          - ceph-mgr-dashboard
          - ceph-mgr-dbg
          - ceph-mgr-diskprediction-cloud
          - ceph-mgr-diskprediction-local
          - ceph-mgr-k8sevents
          - ceph-mgr-rook
          - ceph-mgr-ssh
          - ceph-mon
          - ceph-mon-dbg
          - ceph-osd
          - ceph-osd-dbg
          - ceph-resource-agents
          - ceph-test
          - ceph-test-dbg
          - cephfs-shell
          - libcephfs-dev
          - libcephfs-java
          - libcephfs-jni
          - libcephfs2
          - libcephfs2-dbg
          - librados-dev
          - librados2
          - librados2-dbg
          - libradospp-dev
          - libradosstriper-dev
          - libradosstriper1
          - libradosstriper1-dbg
          - librbd-dev
          - librbd1
          - librbd1-dbg
          - librgw-dev
          - librgw2
          - librgw2-dbg
          - python-ceph
          - python-ceph-argparse
          - python-cephfs
          - python-cephfs-dbg
          - python-rados
          - python-rados-dbg
          - python-rbd
          - python-rbd-dbg
          - python-rgw
          - python-rgw-dbg
          - python3-ceph-argparse
          - python3-cephfs
          - python3-cephfs-dbg
          - python3-rados
          - python3-rados-dbg
          - python3-rbd
          - python3-rbd-dbg
          - python3-rgw
          - python3-rgw-dbg
          - rados-objclass-dev
          - radosgw
          - radosgw-dbg
          - rbd-fuse
          - rbd-fuse-dbg
          - rbd-mirror
          - rbd-mirror-dbg
          - rbd-nbd
          - rbd-nbd-dbg
        purge: yes
        state: absent
      when: "'teardown' in ansible_run_tags"

- hosts: all
  tasks:
    # we need this task to be executed host by host because the same host
    # can run multiple types of daemons.
    - name: install dependencies
      become: yes
      throttle: 1
      apt:
        cache_valid_time: '{{ 60 * 60 * 24 }}' # cache is valid in a day
        install_recommends: no
        name: apt-transport-https
        state: present
        update_cache: yes

    - name: get architecture of packages dpkg installs
      command: dpkg --print-architecture
      changed_when: False
      register: host_arch

    - name: set preserve_repos property
      set_fact:
        preserve_repos: "{{ host_arch.stdout == 'arm64' }}"

    - name: add Ceph APT repository
      become: yes
      template:
        backup: no
        dest: '{{ item.dest }}'
        force: yes
        mode: 0644
        owner: root
        src: '{{ item.src }}'
      loop:
        - { dest: '/etc/apt/sources.list.d/', src: 'ceph.list' }
        - { dest: '/etc/apt/preferences.d/', src: 'ceph.pref' }
      when: preserve_repos

- hosts: localhost
  vars:
    ceph_exec: >
      {{ ceph_deploy }}
      --overwrite-conf
      --username {{ hostvars[item]['ansible_user'] }}
    ceph_host_fqdn: "{{ hostvars[item]['inventory_hostname'] }}:{{ hostvars[item]['ansible_host'] }}"
  tasks:
    # This creates a ceph configuration file `ceph.conf`,
    # a monitor secret keyring `ceph.mon.keyring`,
    # and a log file `ceph-deploy-ceph.log` for the new cluster.
    - name: start deploying a new cluster
      command: >
        {{ ceph_exec }}
        new {{ ceph_host_fqdn }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mons'] }}"
      changed_when: True

    - name: support single node OSDs
      blockinfile:
        backup: no
        block: |
          #Choose a reasonable crush leaf type.
          #0 for a 1-node cluster.
          #1 for a multi node cluster in a single rack
          #2 for a multi node, multi chassis cluster with multiple hosts in a chassis
          #3 for a multi node cluster with hosts across racks, etc.
          osd crush chooseleaf type = 0
        create: no
        insertafter: EOF
        marker: '# {mark} MBWU-CEPH MANAGED BLOCK 1'
        path: '{{ workdir }}/ceph.conf'
        state: present
      when: groups['osds'] | length == 1

    - name: append customized settings to ceph.conf
      blockinfile:
        backup: no
        block: "{{ lookup('template', 'ceph.conf.j2') }}"
        create: no
        insertafter: EOF
        marker: '# {mark} MBWU-CEPH MANAGED BLOCK 2'
        path: '{{ workdir }}/ceph.conf'
        state: present

    - name: install Ceph packages
      command: >
        {{ ceph_exec }}
        install --dev {{ ceph_version }} --dev-commit {{ ceph_commit }}
        {{ '--no-adjust-repos' if hostvars[item]['preserve_repos'] else '' }}
        {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['all'] }}"
      register: install_ceph_res
      changed_when: install_ceph_res.rc == 0

    - name: deploy Ceph monitors
      command: >
        {{ ceph_exec }}
        mon create {{ ceph_host_fqdn }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mons'] }}"
      changed_when: True

    - name: gather authentication keys for provisioning
      command: >
        {{ ceph_exec }}
        gatherkeys {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mons'] }}"
      changed_when: True

    - name: push configuration and client.admin key to admin nodes
      command: >
        {{ ceph_exec }}
        admin {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['admins'] }}"
      changed_when: True

    - name: deploy Ceph MGRs
      command: >
        {{ ceph_exec }}
        mgr create {{ ceph_host_fqdn }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mgrs'] }}"
      changed_when: True

    - name: create OSDs from data disks
      command: >
        {{ ceph_deploy }} --username {{ item[0]['ansible_user'] }}
        osd create {{ item[0]['ansible_host'] }}
        --data {{ item[1] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['osds'] | map('extract', hostvars) | list | subelements('data_disks') }}"
      changed_when: True

- hosts: admins[0]
  tasks:
    - name: show cluster status
      become: yes
      command: ceph -s
      changed_when: False
