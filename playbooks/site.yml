---
- hosts:
    - all
    - localhost
  tags: [teardown]
  tasks:
    - name: verify the OS distribution
      debug:
        msg: We currently support hosts running with 'Debian' or 'Ubuntu' distribution.
      when: ansible_distribution not in ['Debian', 'Ubuntu']
      failed_when: ansible_distribution not in ['Debian', 'Ubuntu']

- hosts: all
  tasks:
    - name: verify inventory_hostname
      debug:
        msg: '{{ inventory_hostname }} is not a hostname'
      when: inventory_hostname | ipaddr
      failed_when: inventory_hostname | ipaddr

    - name: verify ansible_host
      debug:
        msg: '{{ ansible_host }} is not a valid IP address ({{ inventory_hostname }})'
      when: not ansible_host | ipaddr
      failed_when: not ansible_host | ipaddr

- name: set up installation environment
  hosts: localhost
  tags: [teardown]
  tasks:
    - name: install Python virtualenv package
      become: yes
      apt:
        install_recommends: no
        name: virtualenv
        state: present
        update_cache: yes

    - name: install ceph-deploy
      pip:
        name: ceph-deploy
        state: present
        version: 2.0.1
        virtualenv: '{{ workdir }}/ceph-deploy'
        virtualenv_python: python2.7

    - name: set ceph-deploy binary path
      set_fact:
        ceph_deploy: '{{ workdir }}/ceph-deploy/bin/ceph-deploy'

    - name: copy cephdeploy.conf to workdir
      copy:
        backup: no
        dest: '{{ workdir }}'
        force: yes
        src: cephdeploy.conf

    - name: add hostnames to the local /etc/hosts file
      become: yes
      lineinfile:
        backup: yes
        create: no
        line: "{{ hostvars[item]['ansible_host'] }} {{ item }}  # added by MBWU-Ceph"
        path: /etc/hosts
        regexp: "^{{ hostvars[item]['ansible_host'] }} "
        state: present
      loop: "{{ groups['all'] }}"

    - name: remove Ceph packages from remote hosts and purge all data
      command: >
        {{ ceph_deploy }} --username {{ hostvars[item]['ansible_user'] }}
        purge {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['all'] }}"
      changed_when: True

    - name: purge any Ceph data from /var/lib/ceph
      command: >
        {{ ceph_deploy }} --username {{ hostvars[item]['ansible_user'] }}
        purgedata {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['all'] }}"
      changed_when: True

    - name: remove authentication keys from the local directory
      command: '{{ ceph_deploy }} forgetkeys'
      args:
        chdir: '{{ workdir }}'
      changed_when: True

- hosts: osds
  tags: [teardown]
  roles:
    - rm-lvm-config

- hosts: all
  serial: 1
  tasks:
    # we need this task to be executed host by host because the same host
    # can run multiple types of daemons.
    - name: install dependencies
      become: yes
      apt:
        install_recommends: no
        name: apt-transport-https
        state: present
        update_cache: yes

- hosts: all
  tasks:
    - name: get architecture of packages dpkg installs
      command: dpkg --print-architecture
      changed_when: False
      register: host_arch

    - name: set preserve_repos property
      set_fact:
        preserve_repos: "{{ host_arch.stdout == 'arm64' and ansible_distribution_release == 'bionic' }}"

    - name: add Ceph APT repository
      become: yes
      copy:
        backup: no
        dest: '{{ item.dest }}'
        force: yes
        src: '{{ item.src }}'
      loop:
        - { dest: '/etc/apt/sources.list.d/', src: 'ceph.list' }
        - { dest: '/etc/apt/preferences.d/', src: 'ceph.pref' }
      when: preserve_repos

- hosts: localhost
  vars:
    ceph_exec: >
      {{ ceph_deploy }}
      --overwrite-conf
      --username {{ hostvars[item]['ansible_user'] }}
    ceph_host_fqdn: "{{ hostvars[item]['inventory_hostname'] }}:{{ hostvars[item]['ansible_host'] }}"
  tasks:
    # This creates a ceph configuration file `ceph.conf`,
    # a monitor secret keyring `ceph.mon.keyring`,
    # and a log file `ceph-deploy-ceph.log` for the new cluster.
    - name: start deploying a new cluster
      command: >
        {{ ceph_exec }}
        new {{ ceph_host_fqdn }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mons'] }}"
      changed_when: True

    - name: support single node OSDs
      blockinfile:
        backup: no
        block: |
          #Choose a reasonable crush leaf type.
          #0 for a 1-node cluster.
          #1 for a multi node cluster in a single rack
          #2 for a multi node, multi chassis cluster with multiple hosts in a chassis
          #3 for a multi node cluster with hosts across racks, etc.
          osd crush chooseleaf type = 0
        create: no
        insertafter: EOF
        marker: '# {mark} MBWU-CEPH MANAGED BLOCK 1'
        path: '{{ workdir }}/ceph.conf'
        state: present
      when: groups['osds'] | length == 1

    - name: append customized settings to ceph.conf
      blockinfile:
        backup: no
        block: "{{ lookup('template', 'ceph.conf.j2') }}"
        create: no
        insertafter: EOF
        marker: '# {mark} MBWU-CEPH MANAGED BLOCK 2'
        path: '{{ workdir }}/ceph.conf'
        state: present

    - name: install Ceph packages
      command: >
        {{ ceph_exec }}
        install --release nautilus
        {{ '--no-adjust-repos' if hostvars[item]['preserve_repos'] else '' }}
        {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['all'] }}"
      changed_when: True

    - name: deploy Ceph monitors
      command: >
        {{ ceph_exec }}
        mon create {{ ceph_host_fqdn }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mons'] }}"
      changed_when: True

    - name: gather authentication keys for provisioning
      command: >
        {{ ceph_exec }}
        gatherkeys {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mons'] }}"
      changed_when: True

    - name: push configuration and client.admin key to admin nodes
      command: >
        {{ ceph_exec }}
        admin {{ hostvars[item]['ansible_host'] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['admins'] }}"
      changed_when: True

    - name: deploy Ceph MGRs
      command: >
        {{ ceph_exec }}
        mgr create {{ ceph_host_fqdn }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['mgrs'] }}"
      changed_when: True

    - name: create OSDs from data disks
      command: >
        {{ ceph_deploy }} --username {{ item[0]['ansible_user'] }}
        osd create {{ item[0]['ansible_host'] }}
        --data {{ item[1] }}
      args:
        chdir: '{{ workdir }}'
      loop: "{{ groups['osds'] | map('extract', hostvars) | list | subelements('data_disks') }}"
      changed_when: True

- hosts: admins
  tasks:
    - name: show cluster status
      become: yes
      command: ceph -s
      run_once: True
      changed_when: False
